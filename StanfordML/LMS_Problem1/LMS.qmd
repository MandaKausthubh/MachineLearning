---
title: "Least Mean Square Linear Regression"
format: html
---

# Introduction to Linear Regression

## What is linear regression?
- The data is of the form $(x^{(i)}, y^{(i)})$. Where the $x^{(i)}$ is a n dimensional vector and $y^(i)$ is a real number.
- This is a form of ML where our Hypothesis Set $ℋ$ is the set of all Linear Equations. Where x is an (n+1)-dimensional vector with $x_0 = 1$. And $θ$ is our parameter.
$$
h_{θ}(x) = θ^T x
$$
- The cost function is defined as the following:
$$
J(θ) = \frac {1} {2} ∑_{i=1}^m (y^{(i)} - h_{θ}(x^{(i)})) ^ 2
$$

- Algorithm: Gradient Descent.
```
{
	θ_j := θ_j - α*{∂ J / ∂ θ_j}  // forall j
}
```
where we get that:
$$
\frac {∂ J} {∂ \theta_j} =  ∑_{i=1}^m ( h_{θ}(x^{(i)}) - y^{(i)}) x^{(i)}_j
$$

## Code example of Linear Regression

### Importing Python Libraries
```{python}
import numpy as np
import matplotlib.pyplot as plt
```

### Defining the Gradient Descent Function
```{python}

def GradientDescent(gradient_equation, learning_rate, start, n_iter=100, tolerance=1e-06):
	vector = start
	for _ in range(n_iter):
		diff = -learning_rate * gradient_equation(vector)
		if np.all(np.abs(diff)  <= tolerance):
			break
		vector += diff
	pass
```

### Generating the data:
```{python}

def TargetLine(x):
	return 2*x + 4.5

M = 20
x = np.random.random(M)*10
y = TargetLine(x) + np.random.normal(size=M)

x_axis = np.linspace(start=0,stop=10,num=1000)

plt.scatter(x,y, s=5, color='red', marker='x')
plt.plot(x_axis, TargetLine(x_axis))
```

### Preparing the data
```{python}
x_data = np.dstack((np.ones_like(x), x))[0]
y_data = y
```

### Gradient Descent
```{python}
def Prediction(x, theta):
	return np.inner(theta, x)

theta = np.zeros_like(x_data[0])
learning_rate = 0.001

n_iter = 1000
for _ in range(n_iter):
	old_theta = theta
	for j in range(len(theta)):
		diff = 0
		for i in range(len(x_data)):
			diff += x_data[i][j] * (Prediction(x_data[i], old_theta) - y_data[i])
		theta[j] -= learning_rate * diff

def NewLine(x):
	return theta[0] + theta[1] * x

x_axis = np.linspace(start=0,stop=10,num=1000)
print(theta)

plt.scatter(x,y, s=10, color='red', marker='x')
plt.plot(x_axis, TargetLine(x_axis))
plt.plot(x_axis, NewLine(x_axis))
```

# Why is Linear Regression and Gradiesnt so good? - Normal Form

We start by defining the Gradient: $$∇_{θ} J(θ) = \left(\frac {∂J} {∂ θ_j}\right)_{j=1} ^ n$$
