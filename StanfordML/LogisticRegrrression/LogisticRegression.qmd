---
title: "Logistic Gradient Descent"
format: html
---

# Classification and Logistic Regression:

## Logistic Regression Problem:

This is somewhat related to the binary classification problem $(y ∈ \{0,1\})$, but where give out the probability of it being of the output 1.

- The data we have is of the form of $(x^{(i)}, y^{(i)})$ where x is an n-dimensional vector. (We can pad it with one to make
it into an (n+1) dimensional vector) and y is a {0,1}.

- The parameter we have here is similar to what we have in Linear regression, which is a $θ$. 

- Our prediction is: sigmoid of a linear operation.

$$
\displaylines{
h_θ(x) = g(θ^T x) = \frac{1} {1 + e^{θ^Tx}}
}
$$

What we do is the following:

$$
\displaylines {
L(θ) = ∏_{i=1} ^ m p(y\ |\ x;\theta) \\ 
l(\theta) = log(L(\theta))
}
$$

We can maximize $l(\theta)$:

$$
\frac {∂ l} {∂ θ_j} (θ) = ∑_{i=1}^m(y^{(i)} - h_θ(x^{(i)})) x_j^{(i)}
$$

So what we do is gradient ascent.
